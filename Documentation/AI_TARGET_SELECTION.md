# ðŸ“‹ PLAN B: AGENT TARGET SELECTION - COMPLETE IMPLEMENTATION GUIDE

## ðŸŽ¯ OBJECTIVE
Enable RL agent to learn target selection with explicit observation-action correspondence, curriculum training, and proper debugging infrastructure.

**Core Problem Solved**: Make the agent understand which action shoots which target by encoding observations in action-centric format with probabilistic threat calculations.

---

## ðŸ“¦ PHASE 1: OBSERVATION REDESIGN (Priority: CRITICAL)

### **Goal**: Make action-target correspondence explicit and learnable with W40K combat probabilities

### **File 1: engine/w40k_engine.py**

#### **Change 1.1: Replace _encode_valid_targets() method**
**Location**: Lines ~1850-1950 (search for `def _encode_valid_targets`)

**Delete entire method and replace with**:

```python
def _encode_valid_targets(self, obs: np.ndarray, active_unit: Dict[str, Any], base_idx: int):
    """
    Encode valid targets with EXPLICIT action-target correspondence and W40K probabilities.
    50 floats = 5 actions Ã— 10 features per action
    
    CRITICAL DESIGN: obs[120 + action_offset*10] directly corresponds to action (4 + action_offset)
    Example: 
    - obs[120:130] = features for what happens if agent presses action 4
    - obs[130:140] = features for what happens if agent presses action 5
    
    This creates DIRECT causal relationship for RL learning:
    "When obs[121]=1.0 (high kill_probability), pressing action 4 gives high reward"
    
    Features per action slot (10 floats):
    0. is_valid (1.0 = target exists, 0.0 = no target in this slot)
    1. kill_probability (0.0-1.0, probability to kill target this turn considering dice)
    2. danger_to_me (0.0-1.0, probability target kills ME next turn)
    3. hp_ratio (target HP_CUR / HP_MAX)
    4. distance_normalized (hex_distance / perception_radius)
    5. is_lowest_hp (1.0 if lowest HP among valid targets)
    6. army_weighted_threat (0.0-1.0, strategic priority considering all friendlies by VALUE)
    7. can_be_charged_by_melee (1.0 if friendly melee can reach this target)
    8. optimal_target_score (RewardMapper priority calculation 0.0-1.0)
    9. target_type_match (unit_registry compatibility 0.0-1.0)
    """
    # Get valid targets using shooting handler
    from .phase_handlers import shooting_handlers
    
    current_phase = self.game_state["phase"]
    valid_targets = []
    
    if current_phase == "shoot":
        target_ids = shooting_handlers.shooting_build_valid_target_pool(
            self.game_state, active_unit["id"]
        )
        valid_targets = [
            self._get_unit_by_id(tid) 
            for tid in target_ids 
            if self._get_unit_by_id(tid)
        ]
    elif current_phase == "charge":
        # Get valid charge targets (enemies within charge range)
        if "MOVE" not in active_unit:
            raise KeyError(f"Active unit missing required 'MOVE' field: {active_unit}")
        
        for enemy in self.game_state["units"]:
            if "player" not in enemy or "HP_CUR" not in enemy:
                raise KeyError(f"Enemy unit missing required fields: {enemy}")
            
            if enemy["player"] != active_unit["player"] and enemy["HP_CUR"] > 0:
                if "col" not in enemy or "row" not in enemy:
                    raise KeyError(f"Enemy unit missing required position fields: {enemy}")
                
                distance = self._calculate_hex_distance(
                    active_unit["col"], active_unit["row"],
                    enemy["col"], enemy["row"]
                )
                
                # Max charge = MOVE + 12 (maximum 2d6 roll)
                max_charge = active_unit["MOVE"] + 12
                if distance <= max_charge:
                    valid_targets.append(enemy)
    
    elif current_phase == "fight":
        # Get valid melee targets (enemies within CC_RNG)
        if "CC_RNG" not in active_unit:
            raise KeyError(f"Active unit missing required 'CC_RNG' field: {active_unit}")
        
        for enemy in self.game_state["units"]:
            if "player" not in enemy or "HP_CUR" not in enemy:
                raise KeyError(f"Enemy unit missing required fields: {enemy}")
            
            if enemy["player"] != active_unit["player"] and enemy["HP_CUR"] > 0:
                if "col" not in enemy or "row" not in enemy:
                    raise KeyError(f"Enemy unit missing required position fields: {enemy}")
                
                distance = self._calculate_hex_distance(
                    active_unit["col"], active_unit["row"],
                    enemy["col"], enemy["row"]
                )
                
                if distance <= active_unit["CC_RNG"]:
                    valid_targets.append(enemy)
    
    # CRITICAL: Sort by distance for CONSISTENT ordering across episodes
    # Agent needs same target in same slot each time for learning
    valid_targets.sort(key=lambda t: self._calculate_hex_distance(
        active_unit["col"], active_unit["row"], t["col"], t["row"]
    ))
    
    # Pre-calculate shared metrics for all targets
    all_hps = [t.get("HP_CUR", 1) for t in valid_targets]
    min_hp = min(all_hps) if all_hps else 1
    
    # Encode up to 5 action slots (actions 4-8)
    for action_idx in range(5):
        feature_base = base_idx + action_idx * 10
        
        if action_idx < len(valid_targets):
            target = valid_targets[action_idx]
            
            # Feature 0: Action validity (CRITICAL - tells agent this action works)
            obs[feature_base + 0] = 1.0
            
            # Feature 1: Kill probability (W40K dice mechanics)
            kill_prob = self._calculate_kill_probability(active_unit, target)
            obs[feature_base + 1] = kill_prob
            
            # Feature 2: Danger to me (probability target kills ME next turn)
            danger_prob = self._calculate_danger_probability(active_unit, target)
            obs[feature_base + 2] = danger_prob
            
            # Feature 3: HP ratio (efficiency metric)
            obs[feature_base + 3] = target.get("HP_CUR", 1) / max(1, target.get("HP_MAX", 1))
            
            # Feature 4: Distance (accessibility)
            distance = self._calculate_hex_distance(
                active_unit["col"], active_unit["row"],
                target["col"], target["row"]
            )
            obs[feature_base + 4] = distance / self.perception_radius
            
            # Feature 5: Is lowest HP (finish-off signal)
            is_lowest = 1.0 if target.get("HP_CUR", 999) == min_hp else 0.0
            obs[feature_base + 5] = is_lowest
            
            # Feature 6: Army-wide weighted threat (strategic priority by VALUE)
            army_threat = self._calculate_army_weighted_threat(target, valid_targets)
            obs[feature_base + 6] = army_threat
            
            # Feature 7: Can be charged by melee (coordination signal)
            can_be_charged = 1.0 if self._can_melee_units_charge_target(target) else 0.0
            obs[feature_base + 7] = can_be_charged
            
            # Feature 8: Optimal target score (RewardMapper-based)
            optimal_score = self._calculate_target_optimality_score(active_unit, target, valid_targets)
            obs[feature_base + 8] = optimal_score
            
            # Feature 9: Target type match (unit_registry compatibility)
            type_match = self._calculate_target_type_match(active_unit, target)
            obs[feature_base + 9] = type_match
            
        else:
            # No target in this slot - all zeros (invalid action)
            for j in range(10):
                obs[feature_base + j] = 0.0
```

#### **Change 1.2: Add probability calculation methods**
**Location**: After _encode_valid_targets method (add new methods)

```python
def _calculate_kill_probability(self, shooter: Dict[str, Any], target: Dict[str, Any]) -> float:
    """
    Calculate actual probability to kill target this turn considering W40K dice mechanics.
    
    Considers:
    - Hit probability (RNG_ATK vs d6)
    - Wound probability (RNG_STR vs target T)
    - Save failure probability (target saves vs RNG_AP)
    - Number of shots (RNG_NB)
    - Damage per successful wound (RNG_DMG)
    
    Returns: 0.0-1.0 probability
    """
    current_phase = self.game_state["phase"]
    
    # Determine if using ranged or melee
    if current_phase == "shoot":
        if "RNG_ATK" not in shooter or "RNG_STR" not in shooter or "RNG_DMG" not in shooter:
            raise KeyError(f"Shooter missing required ranged stats: {shooter}")
        if "RNG_NB" not in shooter:
            raise KeyError(f"Shooter missing required 'RNG_NB' field: {shooter}")
        
        hit_target = shooter["RNG_ATK"]
        strength = shooter["RNG_STR"]
        damage = shooter["RNG_DMG"]
        num_attacks = shooter["RNG_NB"]
        ap = shooter.get("RNG_AP", 0)
    else:  # melee
        if "CC_ATK" not in shooter or "CC_STR" not in shooter or "CC_DMG" not in shooter:
            raise KeyError(f"Shooter missing required melee stats: {shooter}")
        if "CC_NB" not in shooter:
            raise KeyError(f"Shooter missing required 'CC_NB' field: {shooter}")
        
        hit_target = shooter["CC_ATK"]
        strength = shooter["CC_STR"]
        damage = shooter["CC_DMG"]
        num_attacks = shooter["CC_NB"]
        ap = shooter.get("CC_AP", 0)
    
    # Hit probability (need hit_target+ on d6)
    p_hit = max(0.0, min(1.0, (7 - hit_target) / 6.0))
    
    # Wound probability (given hit)
    if "T" not in target:
        raise KeyError(f"Target missing required 'T' field: {target}")
    wound_target = self._calculate_wound_target(strength, target["T"])
    p_wound = max(0.0, min(1.0, (7 - wound_target) / 6.0))
    
    # Save failure probability (given wound)
    save_target = self._calculate_save_target(target, ap)
    p_fail_save = max(0.0, min(1.0, (save_target - 1) / 6.0))
    
    # Probability of one attack causing damage
    p_damage_per_attack = p_hit * p_wound * p_fail_save
    
    # Expected damage from all attacks
    expected_damage = num_attacks * p_damage_per_attack * damage
    
    # Probability of killing (normalize by target HP)
    if expected_damage >= target["HP_CUR"]:
        return 1.0  # Will definitely kill
    else:
        # Partial probability based on expected damage
        return min(1.0, expected_damage / target["HP_CUR"])

def _calculate_danger_probability(self, defender: Dict[str, Any], attacker: Dict[str, Any]) -> float:
    """
    Calculate probability that attacker will kill defender on its next turn.
    Works for ANY unit pair (active unit vs enemy, VIP vs enemy, etc.)
    
    Considers:
    - Distance (can they reach?)
    - Hit/wound/save probabilities
    - Number of attacks
    - Damage output
    
    Returns: 0.0-1.0 probability
    """
    distance = self._calculate_hex_distance(
        defender["col"], defender["row"],
        attacker["col"], attacker["row"]
    )
    
    # Determine if attacker can reach defender
    can_use_ranged = distance <= attacker.get("RNG_RNG", 0)
    can_use_melee = distance <= attacker.get("CC_RNG", 0)
    
    if not can_use_ranged and not can_use_melee:
        return 0.0  # Out of range
    
    # Use ranged if available and not in melee range, otherwise melee
    if can_use_ranged and not can_use_melee:
        # Ranged attack
        if "RNG_ATK" not in attacker or "RNG_STR" not in attacker:
            return 0.0  # No ranged capability
        
        hit_target = attacker["RNG_ATK"]
        strength = attacker["RNG_STR"]
        damage = attacker["RNG_DMG"]
        num_attacks = attacker.get("RNG_NB", 0)
        ap = attacker.get("RNG_AP", 0)
    else:
        # Melee attack (or close enough for both, prefer melee)
        if "CC_ATK" not in attacker or "CC_STR" not in attacker:
            return 0.0  # No melee capability
        
        hit_target = attacker["CC_ATK"]
        strength = attacker["CC_STR"]
        damage = attacker["CC_DMG"]
        num_attacks = attacker.get("CC_NB", 0)
        ap = attacker.get("CC_AP", 0)
    
    if num_attacks == 0:
        return 0.0  # Can't attack
    
    # Hit probability
    p_hit = max(0.0, min(1.0, (7 - hit_target) / 6.0))
    
    # Wound probability
    if "T" not in defender:
        return 0.0
    wound_target = self._calculate_wound_target(strength, defender["T"])
    p_wound = max(0.0, min(1.0, (7 - wound_target) / 6.0))
    
    # Save failure probability
    save_target = self._calculate_save_target(defender, ap)
    p_fail_save = max(0.0, min(1.0, (save_target - 1) / 6.0))
    
    # Expected damage
    p_damage_per_attack = p_hit * p_wound * p_fail_save
    expected_damage = num_attacks * p_damage_per_attack * damage
    
    # Probability of killing defender
    if expected_damage >= defender["HP_CUR"]:
        return 1.0
    else:
        return min(1.0, expected_damage / defender["HP_CUR"])

def _calculate_army_weighted_threat(self, target: Dict[str, Any], valid_targets: List[Dict[str, Any]]) -> float:
    """
    Calculate army-wide weighted threat score considering all friendly units by VALUE.
    
    This is the STRATEGIC PRIORITY feature that teaches the agent to:
    - Protect high-VALUE units (Leaders, Elites)
    - Consider threats to the entire team, not just personal survival
    - Make sacrifices when strategically necessary
    
    Logic:
    1. For each friendly unit, calculate danger from this target
    2. Weight that danger by the friendly unit's VALUE (1-200)
    3. Sum all weighted dangers
    4. Normalize to 0.0-1.0 based on highest threat among all targets
    
    Example:
    - Target threatens Leader (VALUE=100) with 0.8 danger â†’ 80.0 weighted threat
    - Target threatens Troop (VALUE=10) with 0.9 danger â†’ 9.0 weighted threat
    - Total: 89.0 weighted threat score
    
    Returns: 0.0-1.0 (1.0 = highest strategic threat among all targets)
    """
    my_player = self.game_state["current_player"]
    friendly_units = [
        u for u in self.game_state["units"]
        if u["player"] == my_player and u["HP_CUR"] > 0
    ]
    
    if not friendly_units:
        return 0.0
    
    # Calculate weighted threat for THIS target
    total_weighted_threat = 0.0
    for friendly in friendly_units:
        danger = self._calculate_danger_probability(friendly, target)
        unit_value = friendly.get("VALUE", 10.0)  # Default VALUE if missing
        
        # Weight danger by unit VALUE (1-200 range)
        weighted_threat = danger * unit_value
        total_weighted_threat += weighted_threat
    
    # Calculate weighted threats for ALL targets to find maximum
    all_weighted_threats = []
    for t in valid_targets:
        t_total = 0.0
        for friendly in friendly_units:
            danger = self._calculate_danger_probability(friendly, t)
            unit_value = friendly.get("VALUE", 10.0)
            t_total += danger * unit_value
        all_weighted_threats.append(t_total)
    
    max_weighted_threat = max(all_weighted_threats) if all_weighted_threats else 1.0
    
    # Normalize to 0.0-1.0 range
    if max_weighted_threat > 0:
        return min(1.0, total_weighted_threat / max_weighted_threat)
    else:
        return 0.0

def _calculate_target_optimality_score(self, active_unit: Dict[str, Any], 
                                      target: Dict[str, Any], 
                                      all_targets: List[Dict[str, Any]]) -> float:
    """
    Calculate RewardMapper-based optimality score for target (0.0-1.0).
    Uses shooting_priority_reward logic without actual reward calculation.
    Higher score = better target according to tactical priorities.
    """
    try:
        reward_mapper = self._get_reward_mapper()
        enriched_unit = self._enrich_unit_for_reward_mapper(active_unit)
        enriched_target = self._enrich_unit_for_reward_mapper(target)
        enriched_all = [self._enrich_unit_for_reward_mapper(t) for t in all_targets]
        
        # Get priority reward (higher = better target)
        can_melee_charge = self._can_melee_units_charge_target(target)
        priority_reward = reward_mapper.get_shooting_priority_reward(
            enriched_unit, enriched_target, enriched_all, can_melee_charge
        )
        
        # Normalize to 0.0-1.0 range (assume rewards -1.0 to +3.0)
        normalized = (priority_reward + 1.0) / 4.0
        return np.clip(normalized, 0.0, 1.0)
        
    except Exception as e:
        # Fallback: simple threat-based score
        threat = max(target.get("RNG_DMG", 0), target.get("CC_DMG", 0))
        return min(1.0, threat / 5.0)

def _calculate_target_type_match(self, active_unit: Dict[str, Any], 
                                target: Dict[str, Any]) -> float:
    """
    Calculate unit_registry-based type compatibility (0.0-1.0).
    Higher = this unit is specialized against this target type.
    
    Example: RangedSwarm unit gets 1.0 against Swarm targets, 0.3 against others
    """
    try:
        if not hasattr(self, 'unit_registry') or not self.unit_registry:
            return 0.5  # Neutral
        
        # Get attack preference from unit type name
        unit_type = active_unit.get("unitType", "")
        
        # Parse attack preference (e.g., "RangedSwarm" -> prefers Swarm targets)
        if "Swarm" in unit_type:
            preferred = "swarm"
        elif "Troop" in unit_type:
            preferred = "troop"
        elif "Elite" in unit_type:
            preferred = "elite"
        elif "Leader" in unit_type:
            preferred = "leader"
        else:
            return 0.5  # Neutral
        
        # Get target's defensive type based on HP
        target_hp = target.get("HP_MAX", 1)
        if target_hp <= 1:
            target_type = "swarm"
        elif target_hp <= 3:
            target_type = "troop"
        elif target_hp <= 6:
            target_type = "elite"
        else:
            target_type = "leader"
        
        # Return match score
        return 1.0 if preferred == target_type else 0.3
        
    except Exception:
        return 0.5  # Neutral on error
```

---

## ðŸ“¦ PHASE 2: ACTION MASKING ENHANCEMENT (Priority: HIGH)

### **Goal**: Ensure action mask perfectly aligns with observation encoding

### **File 2: engine/w40k_engine.py**

#### **Change 2.1: Add mask-observation debug logging**
**Location**: In `get_action_mask()` method, after shooting phase masking logic (around line 1260)

**Find this code**:
```python
        elif current_phase == "shoot":
            # ... existing masking code ...
            mask[11] = True  # Wait always valid
```

**Replace with**:
```python
        elif current_phase == "shoot":
            # Shooting phase: actions 4-8 (target slots 0-4) + 11 (wait)
            active_unit = eligible_units[0] if eligible_units else None
            if active_unit:
                from .phase_handlers import shooting_handlers
                valid_targets = shooting_handlers.shooting_build_valid_target_pool(
                    self.game_state, active_unit["id"]
                )
                num_targets = len(valid_targets)
                
                if num_targets > 0:
                    for i in range(min(5, num_targets)):
                        mask[4 + i] = True
                    
                    # CRITICAL DEBUG: Verify mask-observation alignment (first 3 episodes only)
                    if self.game_state["turn"] <= 3 and self.game_state["episode_steps"] < 30:
                        obs = self._build_observation()
                        print(f"\nðŸ” MASK-OBSERVATION ALIGNMENT CHECK:")
                        print(f"   Turn {self.game_state['turn']}, Unit {active_unit['id']}, Targets: {num_targets}")
                        print(f"   Mask enabled actions: {[i for i in range(12) if mask[i]]}")
                        
                        for i in range(min(5, num_targets)):
                            obs_base = 120 + i * 10
                            is_valid = obs[obs_base + 0]
                            kill_prob = obs[obs_base + 1]
                            danger = obs[obs_base + 2]
                            army_threat = obs[obs_base + 6]
                            print(f"   Action {4+i}: valid={is_valid:.1f}, kill_prob={kill_prob:.2f}, danger={danger:.2f}, army_threat={army_threat:.2f}")
                        
                        if not hasattr(self, '_alignment_verified'):
                            self._alignment_verified = True
                            print(f"   âœ… Alignment verified: {num_targets} targets = {num_targets} valid actions")
            
            mask[11] = True  # Wait always valid
```

---

## ðŸ“¦ PHASE 3: CURRICULUM TRAINING CONFIGS (Priority: CRITICAL)

### **Goal**: Progressive reward shaping to teach target selection incrementally

### **File 3: config/training_config.json**

#### **Change 3.1: Add curriculum training configurations**

**Add these THREE new config blocks after the "debug" config**:

```json
  "curriculum_phase1": {
    "observation_params": {
      "obs_size": 170,
      "perception_radius": 25,
      "max_nearby_units": 10,
      "max_valid_targets": 5,
      "justification": "Phase 1: Learn 'shooting is good' - any target, high exploration"
    },
    "model_params": {
      "policy": "MlpPolicy",
      "learning_rate": 0.001,
      "n_steps": 512,
      "batch_size": 32,
      "n_epochs": 4,
      "gamma": 0.95,
      "gae_lambda": 0.9,
      "clip_range": 0.2,
      "ent_coef": 0.20,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "tensorboard_log": "./tensorboard/",
      "policy_kwargs": {
        "net_arch": [128, 128]
      },
      "verbose": 0
    },
    "total_episodes": 100,
    "max_turns_per_episode": 5,
    "max_steps_per_turn": 250,
    "callback_params": {
      "checkpoint_save_freq": 2500,
      "checkpoint_name_prefix": "ppo_curriculum_p1",
      "eval_deterministic": true,
      "eval_render": false,
      "n_eval_episodes": 5
    }
  },
  "curriculum_phase2": {
    "observation_params": {
      "obs_size": 170,
      "perception_radius": 25,
      "max_nearby_units": 10,
      "max_valid_targets": 5,
      "justification": "Phase 2: Learn target priorities - kill weak targets first"
    },
    "model_params": {
      "policy": "MlpPolicy",
      "learning_rate": 0.0005,
      "n_steps": 1024,
      "batch_size": 64,
      "n_epochs": 6,
      "gamma": 0.95,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "ent_coef": 0.10,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "tensorboard_log": "./tensorboard/",
      "policy_kwargs": {
        "net_arch": [256, 256]
      },
      "verbose": 0
    },
    "total_episodes": 500,
    "max_turns_per_episode": 5,
    "max_steps_per_turn": 250,
    "callback_params": {
      "checkpoint_save_freq": 10000,
      "checkpoint_name_prefix": "ppo_curriculum_p2",
      "eval_deterministic": true,
      "eval_render": false,
      "n_eval_episodes": 10
    }
  },
  "curriculum_phase3": {
    "observation_params": {
      "obs_size": 170,
      "perception_radius": 25,
      "max_nearby_units": 10,
      "max_valid_targets": 5,
      "justification": "Phase 3: Learn full tactical priorities with RewardMapper"
    },
    "model_params": {
      "policy": "MlpPolicy",
      "learning_rate": 0.0003,
      "n_steps": 2048,
      "batch_size": 64,
      "n_epochs": 10,
      "gamma": 0.95,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "ent_coef": 0.05,
      "vf_coef": 0.5,
      "max_grad_norm": 0.5,
      "tensorboard_log": "./tensorboard/",
      "policy_kwargs": {
        "net_arch": [256, 256]
      },
      "verbose": 0
    },
    "total_episodes": 1000,
    "max_turns_per_episode": 5,
    "max_steps_per_turn": 250,
    "callback_params": {
      "checkpoint_save_freq": 20000,
      "checkpoint_name_prefix": "ppo_curriculum_p3",
      "eval_deterministic": true,
      "eval_render": false,
      "n_eval_episodes": 10
    }
  }
```

---

## ðŸ“¦ PHASE 4: CURRICULUM REWARD SHAPING (Priority: CRITICAL)

### **Goal**: Create simplified reward configs for each training phase

### **File 4: config/rewards_config.json**

#### **Change 4.1: Add curriculum reward configs**

**Add these THREE new reward configs (one per curriculum phase)**:

**Find the existing agent config** (e.g., `"SpaceMarine_Infantry_Troop_RangedSwarm"`), **copy it**, and create three variants:

```json
  "SpaceMarine_Infantry_Troop_RangedSwarm_CURRICULUM_P1": {
    "base_actions": {
      "move_close": 0.2,
      "move_away": -0.1,
      "move_to_charge": 0.3,
      "move_to_los": 0.3,
      "ranged_attack": 5.0,
      "charge_success": 0.5,
      "melee_attack": 0.5,
      "wait": -1.0
    },
    "result_bonuses": {
      "hit_target": 0.0,
      "wound_target": 0.0,
      "damage_target": 0.0,
      "kill_target": 10.0,
      "no_overkill": 0.0
    },
    "situational_modifiers": {
      "win": 50.0,
      "lose": -50.0,
      "draw": 0.0
    },
    "target_type_bonuses": {},
    "shoot_priority_1": 0.0,
    "shoot_priority_2": 0.0,
    "shoot_priority_3": 0.0,
    "charge_priority_1": 0.0,
    "charge_priority_2": 0.0,
    "charge_priority_3": 0.0,
    "attack_priority_1": 0.0,
    "attack_priority_2": 0.0
  },
  "SpaceMarine_Infantry_Troop_RangedSwarm_CURRICULUM_P2": {
    "base_actions": {
      "move_close": 0.2,
      "move_away": -0.1,
      "move_to_charge": 0.3,
      "move_to_los": 0.3,
      "ranged_attack": 2.0,
      "charge_success": 0.5,
      "melee_attack": 0.5,
      "wait": -0.5
    },
    "result_bonuses": {
      "hit_target": 1.0,
      "wound_target": 2.0,
      "damage_target": 3.0,
      "kill_target": 10.0,
      "no_overkill": 2.0
    },
    "situational_modifiers": {
      "win": 50.0,
      "lose": -50.0,
      "draw": 0.0
    },
    "target_type_bonuses": {
      "vs_swarm": 2.0,
      "vs_troop": 0.0,
      "vs_elite": -1.0,
      "vs_leader": -2.0
    },
    "shoot_priority_1": 3.0,
    "shoot_priority_2": 2.0,
    "shoot_priority_3": 1.0,
    "charge_priority_1": 0.0,
    "charge_priority_2": 0.0,
    "charge_priority_3": 0.0,
    "attack_priority_1": 0.0,
    "attack_priority_2": 0.0
  },
  "SpaceMarine_Infantry_Troop_RangedSwarm_CURRICULUM_P3": {
    "base_actions": {
      "move_close": 0.4,
      "move_away": -0.2,
      "move_to_charge": 0.6,
      "move_to_los": 0.8,
      "ranged_attack": 1.0,
      "charge_success": 0.8,
      "melee_attack": 0.6,
      "wait": -0.3
    },
    "result_bonuses": {
      "hit_target": 0.5,
      "wound_target": 1.0,
      "damage_target": 1.5,
      "kill_target": 3.0,
      "no_overkill": 0.5
    },
    "situational_modifiers": {
      "win": 100.0,
      "lose": -100.0,
      "draw": 0.0
    },
    "target_type_bonuses": {
      "vs_swarm": 1.5,
      "vs_troop": 0.5,
      "vs_elite": -0.5,
      "vs_leader": -1.0,
      "vs_ranged": 0.3,
      "vs_melee": -0.3
    },
    "shoot_priority_1": 2.0,
    "shoot_priority_2": 1.5,
    "shoot_priority_3": 1.0,
    "charge_priority_1": 1.5,
    "charge_priority_2": 1.0,
    "charge_priority_3": 0.5,
    "attack_priority_1": 1.5,
    "attack_priority_2": 1.0
  }
```

---

## ðŸ“¦ PHASE 5: TRAINING EXECUTION (Priority: HIGH)

### **Goal**: Execute curriculum training in sequence

### **Commands to run (in order)**:

#### **Phase 1: Learn "Shooting is Good" (100 episodes, ~30 minutes)**
```bash
python ai/train.py \
  --training-config curriculum_phase1 \
  --rewards-config default \
  --new \
  --test-episodes 5

# Expected behavior:
# - Episodes 1-30: Random exploration, tries all actions
# - Episodes 30-70: Starts preferring shooting actions (4-8) over wait (11)
# - Episodes 70-100: Consistently shoots when targets available
# 
# Success metric: Agent uses action 4-8 in >80% of shooting phases
```

#### **Phase 2: Learn "Target Priority Matters" (500 episodes, ~3 hours)**
```bash
python ai/train.py \
  --training-config curriculum_phase2 \
  --rewards-config default \
  --append \
  --test-episodes 10

# Expected behavior:
# - Episodes 100-300: Explores different target selections
# - Episodes 300-400: Learns to kill weak targets first (kill_prob=1.0)
# - Episodes 400-600: Learns threat prioritization
#
# Success metric: Agent achieves >60% optimal target selection rate
```

#### **Phase 3: Learn "Full Tactical Priorities" (1000 episodes, ~6 hours)**
```bash
python ai/train.py \
  --training-config curriculum_phase3 \
  --rewards-config default \
  --append \
  --test-episodes 20

# Expected behavior:
# - Episodes 600-1000: Refines target selection strategy
# - Episodes 1000-1500: Learns RewardMapper priorities
# - Episodes 1500-1600: Optimizes exploitation vs exploration
#
# Success metric: Agent matches or exceeds handler auto-selection performance
```

---

## ðŸ“¦ PHASE 6: DEBUGGING INFRASTRUCTURE (Priority: MEDIUM)

### **Goal**: Monitor and diagnose training issues

### **File 5: Create new file `ai/target_selection_monitor.py`**

```python
#!/usr/bin/env python3
"""
Target selection monitoring and analysis tool.
Tracks which targets agent selects and compares to optimal choices.
"""

import json
import numpy as np
from collections import defaultdict
from typing import Dict, List, Any

class TargetSelectionMonitor:
    """Monitor agent's target selection decisions during training."""
    
    def __init__(self, output_file="target_selection_analysis.json"):
        self.output_file = output_file
        self.episode_selections = []
        self.current_episode = {
            "selections": [],
            "optimal_count": 0,
            "total_count": 0
        }
    
    def log_selection(self, unit_id: str, selected_action: int, 
                     available_targets: List[Dict], observation: np.ndarray):
        """Log a single target selection decision."""
        
        # Extract observation features for selected action
        action_offset = selected_action - 4  # Convert action 4-8 to offset 0-4
        obs_base = 120 + action_offset * 10
        
        selected_features = {
            "is_valid": float(observation[obs_base + 0]),
            "kill_probability": float(observation[obs_base + 1]),
            "danger_to_me": float(observation[obs_base + 2]),
            "hp_ratio": float(observation[obs_base + 3]),
            "distance": float(observation[obs_base + 4]),
            "is_lowest_hp": float(observation[obs_base + 5]),
            "army_weighted_threat": float(observation[obs_base + 6]),
            "can_be_charged": float(observation[obs_base + 7]),
            "optimal_score": float(observation[obs_base + 8]),
            "type_match": float(observation[obs_base + 9])
        }
        
        # Determine if selection was optimal (highest army_weighted_threat)
        all_army_threats = []
        for i in range(5):
            obs_idx = 120 + i * 10
            if observation[obs_idx] > 0.5:  # Valid target
                all_army_threats.append((i, observation[obs_idx + 6]))
        
        is_optimal = False
        if all_army_threats:
            best_action_offset = max(all_army_threats, key=lambda x: x[1])[0]
            is_optimal = (action_offset == best_action_offset)
        
        self.current_episode["selections"].append({
            "unit_id": unit_id,
            "action": selected_action,
            "features": selected_features,
            "is_optimal": is_optimal
        })
        
        self.current_episode["total_count"] += 1
        if is_optimal:
            self.current_episode["optimal_count"] += 1
    
    def end_episode(self, episode_num: int, total_reward: float, winner: int):
        """Finalize current episode and calculate statistics."""
        
        optimal_rate = 0.0
        if self.current_episode["total_count"] > 0:
            optimal_rate = self.current_episode["optimal_count"] / self.current_episode["total_count"]
        
        self.episode_selections.append({
            "episode": episode_num,
            "total_reward": total_reward,
            "winner": winner,
            "selections": self.current_episode["selections"],
            "optimal_rate": optimal_rate,
            "selection_count": self.current_episode["total_count"]
        })
        
        # Reset for next episode
        self.current_episode = {
            "selections": [],
            "optimal_count": 0,
            "total_count": 0
        }
        
        # Print progress every 10 episodes
        if episode_num % 10 == 0:
            recent_rates = [ep["optimal_rate"] for ep in self.episode_selections[-10:]]
            avg_rate = sum(recent_rates) / len(recent_rates) if recent_rates else 0.0
            print(f"Episode {episode_num}: Optimal target selection rate: {avg_rate:.1%}")
    
    def save_analysis(self):
        """Save complete analysis to JSON file."""
        with open(self.output_file, 'w') as f:
            json.dump({
                "episodes": self.episode_selections,
                "summary": self._calculate_summary()
            }, f, indent=2)
    
    def _calculate_summary(self) -> Dict[str, Any]:
        """Calculate overall training summary statistics."""
        if not self.episode_selections:
            return {}
        
        all_rates = [ep["optimal_rate"] for ep in self.episode_selections]
        
        # Calculate moving average for trend analysis
        window_size = 50
        moving_avg = []
        for i in range(len(all_rates) - window_size + 1):
            window = all_rates[i:i+window_size]
            moving_avg.append(sum(window) / window_size)
        
        return {
            "total_episodes": len(self.episode_selections),
            "overall_optimal_rate": sum(all_rates) / len(all_rates),
            "final_50_episode_rate": sum(all_rates[-50:]) / min(50, len(all_rates)),
            "best_rate": max(all_rates),
            "worst_rate": min(all_rates),
            "improvement_trend": moving_avg[-1] - moving_avg[0] if len(moving_avg) > 0 else 0.0
        }
```

---

## ðŸ“¦ PHASE 7: VERIFICATION & TESTING (Priority: HIGH)

### **Goal**: Validate that target selection actually works

### **Test Script: Create `test_target_selection.py`**

```python
#!/usr/bin/env python3
"""
Test script to verify target selection is working correctly.
"""

import numpy as np
from engine.w40k_engine import W40KEngine
from stable_baselines3 import PPO

def test_observation_action_correspondence():
    """Test that observation encoding matches action space."""
    print("ðŸ§ª Testing observation-action correspondence...")
    
    # Create engine
    engine = W40KEngine(
        rewards_config="default",
        training_config_name="curriculum_phase1",
        scenario_file="config/scenario.json"
    )
    
    obs, info = engine.reset()
    
    # Check observation structure
    print(f"âœ… Observation shape: {obs.shape} (expected: (170,))")
    assert obs.shape == (170,), "Observation size mismatch!"
    
    # Check target encoding
    for i in range(5):
        obs_base = 120 + i * 10
        is_valid = obs[obs_base + 0]
        kill_prob = obs[obs_base + 1]
        danger = obs[obs_base + 2]
        army_threat = obs[obs_base + 6]
        print(f"   Action {4+i}: valid={is_valid:.1f}, kill_prob={kill_prob:.2f}, danger={danger:.2f}, army_threat={army_threat:.2f}")
    
    print("âœ… Observation structure verified\n")

def test_action_masking():
    """Test that action masking aligns with observation."""
    print("ðŸ§ª Testing action masking alignment...")
    
    engine = W40KEngine(
        rewards_config="default",
        training_config_name="curriculum_phase1",
        scenario_file="config/scenario.json"
    )
    
    obs, info = engine.reset()
    mask = engine.get_action_mask()
    
    # Verify mask matches observation
    for i in range(5):
        obs_base = 120 + i * 10
        is_valid_obs = obs[obs_base + 0] > 0.5
        is_valid_mask = mask[4 + i]
        
        match = "âœ…" if is_valid_obs == is_valid_mask else "âŒ"
        print(f"   Action {4+i}: obs={is_valid_obs}, mask={is_valid_mask} {match}")
        
        assert is_valid_obs == is_valid_mask, f"Mismatch for action {4+i}!"
    
    print("âœ… Action masking alignment verified\n")

def test_target_selection_execution():
    """Test that agent can actually select different targets."""
    print("ðŸ§ª Testing target selection execution...")
    
    engine = W40KEngine(
        rewards_config="default",
        training_config_name="curriculum_phase1",
        scenario_file="config/scenario.json"
    )
    
    obs, info = engine.reset()
    
    # Try selecting each valid target
    mask = engine.get_action_mask()
    valid_shoot_actions = [i for i in range(4, 9) if mask[i]]
    
    print(f"   Valid shooting actions: {valid_shoot_actions}")
    
    for action in valid_shoot_actions[:2]:  # Test first 2 targets
        obs, info = engine.reset()
        obs, reward, done, truncated, info = engine.step(action)
        print(f"   Action {action}: reward={reward:.2f}, success={info.get('success', False)}")
    
    print("âœ… Target selection execution verified\n")

def test_probability_calculations():
    """Test that probability calculations work correctly."""
    print("ðŸ§ª Testing probability calculations...")
    
    engine = W40KEngine(
        rewards_config="default",
        training_config_name="curriculum_phase1",
        scenario_file="config/scenario.json"
    )
    
    obs, info = engine.reset()
    
    # Check that probabilities are in valid range
    for i in range(5):
        obs_base = 120 + i * 10
        is_valid = obs[obs_base + 0]
        
        if is_valid > 0.5:
            kill_prob = obs[obs_base + 1]
            danger = obs[obs_base + 2]
            army_threat = obs[obs_base + 6]
            
            assert 0.0 <= kill_prob <= 1.0, f"kill_prob out of range: {kill_prob}"
            assert 0.0 <= danger <= 1.0, f"danger out of range: {danger}"
            assert 0.0 <= army_threat <= 1.0, f"army_threat out of range: {army_threat}"
            
            print(f"   Action {4+i}: Probabilities valid âœ…")
    
    print("âœ… Probability calculations verified\n")

if __name__ == "__main__":
    print("=" * 50)
    print("TARGET SELECTION VERIFICATION TESTS")
    print("=" * 50 + "\n")
    
    try:
        test_observation_action_correspondence()
        test_action_masking()
        test_target_selection_execution()
        test_probability_calculations()
        
        print("=" * 50)
        print("ðŸŽ‰ ALL TESTS PASSED!")
        print("=" * 50)
        
    except Exception as e:
        print(f"\nâŒ TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
```

**Run verification**:
```bash
python test_target_selection.py
```

---

## ðŸ“¦ PHASE 8: EXPECTED RESULTS & SUCCESS METRICS

### **Phase 1 Success Criteria** (100 episodes):
- âœ… Agent uses shooting actions (4-8) in >80% of shooting phases
- âœ… Wait penalty drives agent away from action 11
- âœ… Tensorboard shows: `train/action_4_frequency` increasing

### **Phase 2 Success Criteria** (500 episodes):
- âœ… Agent achieves >60% optimal target selection rate
- âœ… Preferentially targets low-HP enemies (kill_prob=1.0)
- âœ… Win rate improves to >40%

### **Phase 3 Success Criteria** (1000 episodes):
- âœ… Agent achieves >80% optimal target selection rate
- âœ… Win rate reaches >60%
- âœ… Matches or exceeds handler auto-selection performance
- âœ… Learns to protect high-VALUE units (Leaders, Elites)

### **Tensorboard Metrics to Monitor**:
```
train/entropy          # Should decrease over time (0.20 â†’ 0.05)
train/action_4_freq    # Should increase (shows agent using shoot)
train/action_5_freq    # Should vary (shows target selection)
eval/win_rate          # Should increase steadily
eval/optimal_target_%  # Custom metric - target selection accuracy
```

---

## ðŸš¨ TROUBLESHOOTING GUIDE

### **Problem 1: Agent still chooses WAIT (action 11)**
**Diagnosis**: Entropy too low, agent converged to safe action
**Solution**: Increase `ent_coef` to 0.25 in phase 1 config

### **Problem 2: Agent shoots randomly, no target preference**
**Diagnosis**: Observation features not informative enough
**Solution**: Verify `obs[121]` (kill_prob) is actually >0.8 for killable targets

### **Problem 3: Training is too slow**
**Diagnosis**: Too many episodes per phase
**Solution**: Reduce phase 2 to 300 episodes, phase 3 to 600 episodes

### **Problem 4: Agent performance plateaus**
**Diagnosis**: Reward shaping needs adjustment
**Solution**: Increase kill_target bonus in phase 2/3 configs

### **Problem 5: Action mask doesn't match observation**
**Diagnosis**: Target list sorting changed between obs and mask
**Solution**: Verify CONSISTENT distance-based sorting in both places

### **Problem 6: Probabilities always 0.0 or 1.0**
**Diagnosis**: Calculation error in probability methods
**Solution**: Check _calculate_wound_target and _calculate_save_target implementations

---

## ðŸ“Š COMPLETION CHECKLIST

- [ ] Phase 1: Observation redesign implemented
- [ ] Phase 1: Probability calculation methods added
- [ ] Phase 2: Action mask debug logging added
- [ ] Phase 3: Curriculum configs created
- [ ] Phase 4: Curriculum rewards created
- [ ] Phase 5: Phase 1 training completed (100 episodes)
- [ ] Phase 5: Phase 2 training completed (500 episodes)
- [ ] Phase 5: Phase 3 training completed (1000 episodes)
- [ ] Phase 6: Monitoring infrastructure created
- [ ] Phase 7: Verification tests passed
- [ ] Phase 8: Success metrics achieved

---

## ðŸŽ¯ FINAL NOTES

**Expected Total Time**: 
- Implementation: 2-3 hours
- Phase 1 Training: 30 minutes
- Phase 2 Training: 3 hours
- Phase 3 Training: 6 hours
- **Total: ~10-12 hours**

**Key Features Implemented**:
- âœ… Probabilistic kill calculations (W40K dice mechanics)
- âœ… Danger probability (will enemy kill me?)
- âœ… Army-wide weighted threat (protects high-VALUE units 1-200)
- âœ… Action-centric observation encoding
- âœ… Curriculum training (progressive learning)

**When Complete**:
- Agent will intelligently select targets based on W40K probabilities
- Agent will protect high-VALUE allies (Leaders > Elites > Troops)
- Performance will match or exceed handler auto-selection
- System will scale to new mechanics automatically
- True adaptive AI with emergent tactical behavior achieved

**This is the proper long-term solution with probabilistic combat mechanics. Good luck! ðŸš€**