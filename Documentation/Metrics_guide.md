# ğŸ“Š Guide to 0_critical/ Metrics - W40K AI Training

## ğŸ“‘ Table of Contents

### Quick Navigation
- [ğŸ¯ Purpose](#-purpose)
- [ğŸ“ˆ Metric Organization](#-metric-organization)

### Game Performance Metrics
1. [ğŸ® a_bot_eval_combined - Primary Goal Metric](#1-a_bot_eval_combined---primary-goal-metric-)
2. [ğŸ® b_win_rate_100ep - Training Performance](#2-b_win_rate_100ep---training-performance)
3. [ğŸ® c_episode_reward_smooth - Learning Progress](#3-c_episode_reward_smooth---learning-progress)

### PPO Health Metrics
4. [âš™ï¸ d_loss_mean - Overall Training Health](#4-d_loss_mean---overall-training-health)
5. [âš™ï¸ e_explained_variance - Value Function Quality](#5-e_explained_variance---value-function-quality)
6. [âš™ï¸ f_clip_fraction - Policy Update Scale](#6-f_clip_fraction---policy-update-scale)
7. [âš™ï¸ g_approx_kl - Policy Stability](#7-g_approx_kl---policy-stability)
8. [âš™ï¸ h_entropy_loss - Exploration Health](#8-h_entropy_loss---exploration-health)

### Technical Health Metrics
9. [ğŸ”§ i_gradient_norm - Gradient Explosion Detector](#9-i_gradient_norm---gradient-explosion-detector)
10. [ğŸ”§ j_immediate_reward_ratio - Reward Composition](#10-j_immediate_reward_ratio---reward-composition)

### Troubleshooting & Tools
- [ğŸš¨ Common Problem Patterns](#-common-problem-patterns)
  - [Pattern 1: "The Plateau"](#pattern-1-the-plateau)
  - [Pattern 2: "The Collapse"](#pattern-2-the-collapse)
  - [Pattern 3: "The Explosion"](#pattern-3-the-explosion)
  - [Pattern 4: "The Shortcut"](#pattern-4-the-shortcut)
- [ğŸ“‹ Quick Reference Card](#-quick-reference-card)
- [ğŸ“ Training Workflow](#-training-workflow)
- [ğŸ”— Config File Mapping](#-config-file-mapping)
- [ğŸ’¡ Pro Tips](#-pro-tips)
- [ğŸ“ Example Diagnosis Session](#-example-diagnosis-session)
- [ğŸ“ Getting Help](#-getting-help)

---

## ğŸ¯ Purpose

The `0_critical/` namespace contains **10 essential metrics** you need to tune PPO hyperparameters and diagnose training issues. These are the ONLY metrics you should focus on during active training.

**Why "0_critical"?** The `0_` prefix ensures this dashboard sorts **first** in TensorBoard alphabetically, making it your primary view.

---

## ğŸ“ˆ Metric Organization

All metrics use **alphabetical prefixes** (a_, b_, c_, etc.) to control sort order in TensorBoard:

- **a-c**: Game Performance (what you're optimizing FOR)
- **d-h**: PPO Health (how WELL the algorithm is learning)
- **i-j**: Technical Health (catching catastrophic failures)

All metrics are **smoothed** using 20-episode rolling averages for clear trend visualization.

---

## ğŸ® GAME PERFORMANCE METRICS

<details>
<summary><h3>1. `a_bot_eval_combined` - Primary Goal Metric ğŸ¯</h3></summary>

**What it measures:** Win rate against evaluation bots (RandomBot, GreedyBot, DefensiveBot)

**Formula:** `0.2Ã—random + 0.3Ã—greedy + 0.5Ã—defensive`

**Target:** `>0.70` (70% combined win rate)

**Range:** `[0.0 - 1.0]`

**Updates:** Only when bot evaluation runs (controlled by `bot_eval_freq` in config)

**What it tells you:**
- **Primary success metric** - This is your training goal
- Measures agent's ability to beat structured opponents
- Weighted heavily toward DefensiveBot (hardest opponent)

**How to interpret:**
- `<0.40`: âŒ Agent is struggling - fundamental learning issues
- `0.40-0.60`: âš ï¸ Agent is learning but needs improvement
- `0.60-0.70`: âœ… Good progress - agent is competitive
- `>0.70`: ğŸ† Success! Agent beats evaluation bots consistently

**Action if too low:**
- Check if other metrics show learning (win_rate_100ep, episode_reward)
- If other metrics are good but this is low â†’ reward structure mismatch
- If all metrics are low â†’ hyperparameter tuning needed

**Why it's named "a_":** Sorts first - this is your #1 priority metric

---

</details>

<details>
<summary><h3>2. `b_win_rate_100ep` - Training Performance</h3></summary>

**What it measures:** Win rate against training opponent over last 100 episodes

**Target:** `>0.50` (agent wins more than it loses)

**Range:** `[0.0 - 1.0]`

**Updates:** Every episode (100-episode rolling window)

**What it tells you:**
- Agent's performance during normal training
- Fast feedback loop (updates constantly)
- Indicates if basic learning is happening

**How to interpret:**
- `<0.30`: âŒ Agent is losing badly - not learning basics
- `0.30-0.50`: âš ï¸ Agent is competitive but needs work
- `0.50-0.70`: âœ… Agent is winning consistently
- `>0.70`: ğŸ† Agent dominates training opponent

**Action if too low:**
- Check `entropy_loss` - agent might not be exploring
- Check `clip_fraction` - learning might be too slow
- Check `explained_variance` - value function might be broken

**Relationship to `a_bot_eval_combined`:**
- Should be HIGHER than bot_eval (training opponent is easier)
- If this is high but bot_eval is low â†’ training opponent is too weak

---

</details>

<details>
<summary><h3>3. `c_episode_reward_smooth` - Learning Progress</h3></summary>

**What it measures:** Average reward per episode (smoothed over 20 episodes)

**Target:** **Increasing trend** (not a specific number)

**Range:** Depends on your reward structure (typically 50-200 for W40K)

**Updates:** Every episode

**What it tells you:**
- Whether the agent is learning to maximize rewards
- Reward signal strength and consistency
- Combined with win_rate, shows learning quality

**How to interpret:**
- **Flat line**: âš ï¸ No learning - agent stuck at local optimum
- **Increasing**: âœ… Agent is learning and improving
- **Decreasing**: âŒ Catastrophic forgetting or policy collapse
- **Very noisy**: âš ï¸ Reward signal is unstable

**Action if not increasing:**
- Check `immediate_reward_ratio` - might be >0.9 (only learning short-term)
- Check `entropy_loss` - might be too low (stopped exploring)
- Check `clip_fraction` - might be too low (learning too slowly)

**Pro tip:** This should correlate with `win_rate_100ep`. If reward increases but win rate doesn't â†’ reward hacking.

---

</details>

## âš™ï¸ PPO HEALTH METRICS

<details>
<summary><h3>4. `d_loss_mean` - Overall Training Health</h3></summary>

**What it measures:** Combined policy loss + value loss (absolute values)

**Target:** **Decreasing trend** over time, stabilizing above 0

**Range:** Typically 50-200 early, 10-50 stable

**Updates:** Every policy update (~every n_steps)

**What it tells you:**
- Overall learning signal strength
- Whether policy and value functions are converging
- Training stability

**How to interpret:**
- **Very high (>200)**: âš ï¸ Early training - this is normal
- **Increasing**: âŒ Training is unstable - diverging
- **Decreasing steadily**: âœ… Healthy learning
- **Stable and low (<50)**: âœ… Converged
- **Oscillating wildly**: âŒ Learning rate too high or gradient explosion

**Action if unstable:**
- Reduce `learning_rate` (e.g., 0.0003 â†’ 0.0001)
- Check `gradient_norm` - might be >10
- Reduce `n_epochs` if oscillating

---

</details>

<details>
<summary><h3>5. `e_explained_variance` - Value Function Quality</h3></summary>

**What it measures:** How well the value function predicts actual returns

**Target:** `>0.30` (ideally >0.70)

**Range:** `[-1.0 to 1.0]` (negative means worse than predicting zero)

**Updates:** Every policy update

**What it tells you:**
- Whether the critic (value function) understands the environment
- Quality of advantage estimates for policy updates
- Fundamental learning capacity

**How to interpret:**
- `<0.0`: âŒ **CRITICAL** - Value function is completely broken
- `0.0-0.30`: âš ï¸ Value function is weak - poor advantage estimates
- `0.30-0.70`: âœ… Value function is working correctly
- `>0.70`: ğŸ† Excellent value function - high quality learning

**Action if too low (<0.30):**
- **MOST COMMON CAUSE:** Reward signal is too sparse or delayed
- Increase `gamma` (e.g., 0.95 â†’ 0.98) for longer-term planning
- Increase `gae_lambda` (e.g., 0.95 â†’ 0.98)
- Check if rewards are too sparse - might need reward shaping
- Increase `vf_coef` (e.g., 0.5 â†’ 1.0) to prioritize value function training

**Why this matters:**
- PPO uses the value function to compute advantages
- Bad value function â†’ bad advantage estimates â†’ bad policy updates
- If this is broken, NOTHING else will work properly

---

</details>

<details>
<summary><h3>6. `f_clip_fraction` - Policy Update Scale</h3></summary>

**What it measures:** Fraction of policy updates that hit the PPO clipping limit

**Target:** `0.1 - 0.3` (10-30% of updates clipped)

**Range:** `[0.0 - 1.0]`

**Updates:** Every policy update

**What it tells you:**
- How aggressively the policy is being updated
- Whether `learning_rate` is appropriate
- Whether policy changes are too large or too small

**How to interpret:**
- `<0.1`: âš ï¸ **Updates too small** - learning is very slow
  - Policy is barely changing
  - Training will take forever
  
- `0.1-0.3`: âœ… **Optimal range**
  - Policy updates are well-sized
  - Safe, stable learning
  
- `>0.3`: âš ï¸ **Updates too large** - risk of instability
  - Policy changing too fast
  - Risk of catastrophic forgetting

**Action based on value:**

| Value | Problem | Solution |
|-------|---------|----------|
| <0.05 | Learning glacially slow | **Increase** `learning_rate` by 2x (0.0003 â†’ 0.0006) |
| 0.05-0.1 | Learning slowly | **Increase** `learning_rate` by 1.5x |
| 0.1-0.3 | âœ… Perfect | No change needed |
| 0.3-0.5 | Aggressive updates | **Decrease** `learning_rate` by 0.7x |
| >0.5 | Too aggressive | **Decrease** `learning_rate` by 0.5x |

**Why this matters:**
- PPO clips policy updates to prevent catastrophic changes
- If most updates are clipped â†’ learning rate is too high
- If no updates are clipped â†’ learning rate is too low

---

</details>

<details>
<summary><h3>7. `g_approx_kl` - Policy Stability</h3></summary>

**What it measures:** KL divergence between old and new policy (how much policy changed)

**Target:** `<0.02` (ideally <0.01 for stable training)

**Range:** `[0.0 - âˆ]` (practically 0.0-0.1)

**Updates:** Every policy update

**What it tells you:**
- How much the policy distribution is changing each update
- Training stability indicator
- Whether `target_kl` constraint is working

**How to interpret:**
- `<0.01`: âœ… Very stable, safe learning
- `0.01-0.02`: âœ… Healthy learning pace
- `0.02-0.05`: âš ï¸ Policy changing quickly - monitor closely
- `>0.05`: âŒ **DANGER** - policy changing too fast, risk of collapse

**Action if too high (>0.02):**
- **Decrease `learning_rate`** - primary control
- Set or decrease `target_kl` (e.g., `null` â†’ `0.03` or `0.03` â†’ `0.01`)
- Reduce `n_epochs` (e.g., 10 â†’ 6)

**Action if too low (<0.005) AND performance is bad:**
- Policy is stuck - not exploring enough
- Increase `ent_coef` to encourage exploration
- Slightly increase `learning_rate`

**Relationship to `clip_fraction`:**
- Both measure update size but differently
- approx_kl is more precise but harder to interpret
- Use clip_fraction as primary, approx_kl for safety checks

---

</details>

<details>
<summary><h3>8. `h_entropy_loss` - Exploration Health</h3></summary>

**What it measures:** Negative entropy of the policy distribution (higher = more deterministic)

**Target:** `0.5 - 2.0` (depends on action space)

**Range:** `[-âˆ to 0]` (more negative = more deterministic)

**Updates:** Every policy update

**What it tells you:**
- How much the agent is exploring vs exploiting
- Whether the policy has become too deterministic
- Risk of premature convergence

**How to interpret:**
- `>2.0` (less negative than -2.0): âš ï¸ **Too random** - not learning patterns
- `0.5-2.0`: âœ… Healthy exploration/exploitation balance
- `<0.5` (more negative than -0.5): âŒ **Policy collapse** - stopped exploring

**Action if too low (<0.5):**
- **CRITICAL:** Agent has stopped exploring
- **Increase `ent_coef`** significantly (e.g., 0.1 â†’ 0.3)
- This often happens mid-training as policy becomes deterministic
- May need to restart training with higher ent_coef from the start

**Action if too high (>2.0):**
- Agent is too random - not learning
- **Decrease `ent_coef`** (e.g., 0.3 â†’ 0.1)
- Or increase training time - agent hasn't converged yet

**Why this matters:**
- PPO adds an entropy bonus to encourage exploration
- As training progresses, entropy naturally decreases
- But if it drops TOO fast â†’ agent gets stuck in local optimum
- Your phase1 config has `ent_coef: 0.1` which caused this issue

**Pro tip:** Start with high entropy (0.3) early in training, gradually decrease to 0.1 later.

---

</details>

## ğŸ”§ TECHNICAL HEALTH METRICS

<details>
<summary><h3>9. `i_gradient_norm` - Gradient Explosion Detector</h3></summary>

**What it measures:** L2 norm of the gradients during backpropagation

**Target:** `<10.0` (ideally 1.0-5.0)

**Range:** `[0.0 - âˆ]`

**Updates:** Every policy update

**What it tells you:**
- Whether gradients are stable or exploding
- Technical health of the training process
- Whether `max_grad_norm` clipping is working

**How to interpret:**
- `<1.0`: âœ… Very stable gradients
- `1.0-5.0`: âœ… Healthy gradient flow
- `5.0-10.0`: âš ï¸ Gradients getting large - monitor
- `>10.0`: âŒ **Gradient explosion** - training will fail

**Action if too high (>10):**
- **Decrease `max_grad_norm`** (e.g., 0.5 â†’ 0.3)
- **Decrease `learning_rate`** (e.g., 0.0003 â†’ 0.0001)
- Check if reward scale is too large (rewards >1000?)
- May indicate reward structure issues

**Why this matters:**
- Exploding gradients cause training instability
- Can lead to NaN losses and complete training failure
- max_grad_norm clips gradients to prevent this
- If gradient_norm consistently hits max_grad_norm â†’ need to reduce learning rate

**Note:** Some versions of Stable-Baselines3 don't log this metric. If unavailable, it will show as 0.

---

</details>

<details>
<summary><h3>10. `j_immediate_reward_ratio` - Reward Composition</h3></summary>

**What it measures:** Ratio of immediate (base action) rewards to total episode reward

**Target:** `<0.90` (ideally 0.5-0.7)

**Range:** `[0.0 - 1.0]`

**Updates:** Every episode (if reward decomposition is tracked)

**What it tells you:**
- Whether agent is learning long-term strategy or just immediate rewards
- Balance between short-term and long-term thinking
- Quality of reward structure

**How to interpret:**
- `<0.50`: âœ… Agent learning primarily from strategic rewards
- `0.50-0.70`: âœ… Good balance of immediate and strategic rewards
- `0.70-0.90`: âš ï¸ Heavy reliance on immediate rewards
- `>0.90`: âŒ **Only learning immediate rewards** - no strategy

**Action if too high (>0.90):**
- **CRITICAL:** Agent isn't learning long-term strategy
- Increase `gamma` (e.g., 0.95 â†’ 0.98) - look further into future
- Increase strategic reward bonuses in rewards_config.json
- Reduce immediate action rewards
- May need reward redesign

**Why this matters:**
- In W40K, winning requires strategy (positioning, target priority, resource management)
- If agent only learns "shoot nearest enemy" (immediate reward), it won't develop strategy
- Your training showed this at 1.0 â†’ agent learned zero strategy

**Example:**
- Good agent: 30% from shooting actions, 70% from killing enemies, winning battles
- Bad agent: 90% from shooting actions, 10% from actually accomplishing goals

---

</details>

## ğŸš¨ Common Problem Patterns

### Pattern 1: "The Plateau"
**Symptoms:**
- âœ… `explained_variance` = 0.7 (good)
- âœ… `clip_fraction` = 0.15 (good)
- âŒ `episode_reward` flat
- âŒ `win_rate_100ep` stuck at 0.4

**Diagnosis:** Agent is stuck in local optimum

**Solution:**
1. Increase `ent_coef` (0.1 â†’ 0.3) to explore more
2. Increase `learning_rate` slightly
3. May need curriculum learning or reward redesign

---

### Pattern 2: "The Collapse"
**Symptoms:**
- âŒ `entropy_loss` = -1.5 (too deterministic)
- âŒ `win_rate_100ep` decreasing
- âŒ `episode_reward` decreasing
- âœ… `explained_variance` still good

**Diagnosis:** Policy collapse - agent stopped exploring and forgot what it learned

**Solution:**
1. **Restart training** with higher `ent_coef` (0.3 instead of 0.1)
2. Use entropy decay schedule (start 0.3, gradually reduce to 0.1)
3. Reduce `learning_rate` to prevent forgetting

---

### Pattern 3: "The Explosion"
**Symptoms:**
- âŒ `gradient_norm` = 15+ (exploding)
- âŒ `clip_fraction` = 0.8 (way too high)
- âŒ `approx_kl` = 0.1+ (huge policy changes)
- âŒ All metrics become unstable

**Diagnosis:** Training is unstable - updates too large

**Solution:**
1. **Decrease `learning_rate`** immediately (0.0003 â†’ 0.0001)
2. Decrease `max_grad_norm` (0.5 â†’ 0.3)
3. Set `target_kl` = 0.01 to limit policy changes

---

### Pattern 4: "The Shortcut"
**Symptoms:**
- âœ… `win_rate_100ep` = 0.7 (good!)
- âŒ `a_bot_eval_combined` = 0.2 (bad!)
- âŒ `immediate_reward_ratio` = 0.95

**Diagnosis:** Agent learned to game the training system, not the actual game

**Solution:**
1. Training opponent is too weak - agent learned shortcuts
2. Increase strategic reward bonuses
3. Use curriculum learning with harder opponents
4. Redesign rewards to discourage shortcut strategies

---

## ğŸ“‹ Quick Reference Card

**Print this and keep it next to your monitor:**

| Metric | Good Range | Primary Control | Fix if Bad |
|--------|-----------|-----------------|-----------|
| **a_bot_eval_combined** | >0.70 | Reward structure | Tune other metrics first |
| **b_win_rate_100ep** | >0.50 | Agent learning | Check entropy, clip_fraction |
| **c_episode_reward** | Increasing | Reward signal | Check immediate_reward_ratio |
| **d_loss_mean** | Decreasing | training stability | Reduce learning_rate |
| **e_explained_variance** | >0.30 | gamma, gae_lambda | Increase both to 0.98 |
| **f_clip_fraction** | 0.1-0.3 | **learning_rate** | Adjust learning_rate |
| **g_approx_kl** | <0.02 | learning_rate | Reduce learning_rate, set target_kl |
| **h_entropy_loss** | 0.5-2.0 | **ent_coef** | Increase ent_coef to 0.3 |
| **i_gradient_norm** | <10 | max_grad_norm | Reduce max_grad_norm, learning_rate |
| **j_immediate_reward_ratio** | <0.90 | **gamma** | Increase gamma, redesign rewards |

---

## ğŸ“ Training Workflow

**Use this workflow for efficient training:**

### Step 1: Start Training
Monitor: `e_explained_variance`, `i_gradient_norm`
- If explained_variance <0.3 â†’ Stop, increase gamma
- If gradient_norm >10 â†’ Stop, reduce learning_rate

### Step 2: First 100 Episodes
Monitor: `f_clip_fraction`, `h_entropy_loss`
- Adjust learning_rate to get clip_fraction 0.1-0.3
- Ensure entropy_loss stays in 0.5-2.0 range

### Step 3: First Bot Evaluation (~500 episodes)
Monitor: `a_bot_eval_combined`, `j_immediate_reward_ratio`
- If bot_eval <0.4 AND immediate_ratio >0.9 â†’ Reward structure problem
- If bot_eval <0.4 AND entropy_loss <0.5 â†’ Exploration problem

### Step 4: Mid-Training (1000+ episodes)
Monitor: `b_win_rate_100ep`, `c_episode_reward`
- Should both be increasing
- If plateauing â†’ increase ent_coef or change curriculum

### Step 5: Final Evaluation
Monitor: `a_bot_eval_combined`
- Target >0.70
- If not achieved â†’ analyze which pattern above matches

---

## ğŸ”— Config File Mapping