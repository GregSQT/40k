class MetricsCollectionCallback(BaseCallback):
    """Callback to collect training metrics and send to W40KMetricsTracker."""
    
    def __init__(self, metrics_tracker, model, controlled_agent=None, verbose: int = 0):
        super().__init__(verbose)
        self.metrics_tracker = metrics_tracker
        self.model = model
        self.controlled_agent = controlled_agent  # CRITICAL FIX: Store controlled_agent for bot evaluation
        self.episode_count = 0
        self.episode_reward = 0
        self.episode_length = 0
        
        # Initialize episode tracking with ALL metrics
        self.episode_tactical_data = {
            # Combat metrics
            'shots_fired': 0,
            'hits': 0,
            'total_enemies': 0,
            'killed_enemies': 0,
            
            # NEW: Damage tracking
            'damage_dealt': 0,
            'damage_received': 0,
            
            # NEW: Unit tracking
            'units_lost': 0,
            'units_killed': 0,
            
            # NEW: Action tracking
            'valid_actions': 0,
            'invalid_actions': 0,
            'wait_actions': 0,
            'total_actions': 0,
            
            # Phase tracking
            'phases_completed': 0,
            'total_phases': 6
        }
        
        # Track initial unit state for damage/loss calculations
        self.initial_agent_units = []
        self.initial_enemy_units = []
        
        # Add immediate reward ratio history for smoothing
        self.immediate_reward_ratio_history = []
        self.max_reward_ratio_history = 50  # Keep last 50 episodes
        
        # Q-value tracking with history
        self.q_value_history = []
        self.max_q_value_history = 100  # Keep last 100 Q-value samples
    
    def _on_training_start(self) -> None:
        """Called when training starts - update metrics_tracker to use model's logger directory."""
        # CRITICAL FIX: Update metrics_tracker writer to use SB3's actual tensorboard directory
        # The model's logger is now initialized, so we can get the real directory
        if hasattr(self.model, 'logger') and self.model.logger:
            actual_log_dir = self.model.logger.get_dir()
            
            # Close old writer and create new one in correct directory
            self.metrics_tracker.writer.close()
            from torch.utils.tensorboard import SummaryWriter
            self.metrics_tracker.writer = SummaryWriter(actual_log_dir)
            self.metrics_tracker.log_dir = actual_log_dir
    
    def print_final_training_summary(self, model=None, training_config=None, training_config_name=None, rewards_config_name=None):
        """Print comprehensive training summary with final bot evaluation"""
        
        print("\n" + "="*80)
        print("ðŸŽ¯ TRAINING COMPLETE - RUNNING FINAL EVALUATION")
        print("="*80)
        
        if EVALUATION_BOTS_AVAILABLE and model and training_config and training_config_name and rewards_config_name:
            # Extract n_episodes from config
            if 'callback_params' not in training_config:
                raise KeyError("training_config missing required 'callback_params' field")
            if 'bot_eval_final' not in training_config['callback_params']:
                raise KeyError("training_config['callback_params'] missing required 'bot_eval_final' field")
            n_final = training_config['callback_params']['bot_eval_final']
            
            bot_results = self._run_final_bot_eval(model, training_config, training_config_name, rewards_config_name)
            
            if bot_results:
                # Log to metrics_tracker for TensorBoard
                if hasattr(self, 'metrics_tracker') and self.metrics_tracker:
                    self.metrics_tracker.log_bot_evaluations(bot_results)
                    # Flush to ensure metrics are written immediately
                    self.metrics_tracker.writer.flush()
                
                # Print results
                print(f"vs RandomBot:     {bot_results['random']:.2f} ({bot_results['random_wins']}/{n_final} wins)")
                print(f"vs GreedyBot:     {bot_results['greedy']:.2f} ({bot_results['greedy_wins']}/{n_final} wins)")
                print(f"vs DefensiveBot:  {bot_results['defensive']:.2f} ({bot_results['defensive_wins']}/{n_final} wins)")
                print(f"\nCombined Score:   {bot_results['combined']:.2f} {'âœ…' if bot_results['combined'] >= 0.70 else 'âš ï¸'}")
        
        # Critical metrics check
        print(f"\nðŸ“Š CRITICAL METRICS:")
        
        # Check clip_fraction
        if len(self.metrics_tracker.hyperparameter_tracking['clip_fractions']) >= 20:
            recent_clip = np.mean(self.metrics_tracker.hyperparameter_tracking['clip_fractions'][-20:])
            clip_status = "âœ…" if 0.1 <= recent_clip <= 0.3 else "âš ï¸ "
            print(f"   Clip Fraction:      {recent_clip:.3f} {clip_status} (target: 0.1-0.3)")
            if recent_clip < 0.1:
                print(f"      â†’ Increase learning_rate (policy not updating enough)")
            elif recent_clip > 0.3:
                print(f"      â†’ Decrease learning_rate (too aggressive updates)")
        
        # Check explained_variance
        if len(self.metrics_tracker.hyperparameter_tracking['explained_variances']) >= 20:
            recent_ev = np.mean(self.metrics_tracker.hyperparameter_tracking['explained_variances'][-20:])
            ev_status = "âœ…" if recent_ev > 0.3 else "âš ï¸ "
            print(f"   Explained Variance: {recent_ev:.3f} {ev_status} (target: >0.3)")
            if recent_ev < 0.3:
                print(f"      â†’ Value function struggling - check reward signal")
        
        # Check entropy
        if len(self.metrics_tracker.hyperparameter_tracking['entropy_losses']) >= 20:
            recent_entropy = np.mean(self.metrics_tracker.hyperparameter_tracking['entropy_losses'][-20:])
            entropy_status = "âœ…" if 0.5 <= recent_entropy <= 2.0 else "âš ï¸ "
            print(f"   Entropy Loss:       {recent_entropy:.3f} {entropy_status} (target: 0.5-2.0)")
            if recent_entropy < 0.5:
                print(f"      â†’ Increase ent_coef (exploration too low)")
            elif recent_entropy > 2.0:
                print(f"      â†’ Decrease ent_coef (exploration too high)")
        
        # Check gradient_norm
        if hasattr(self.metrics_tracker, 'latest_gradient_norm') and self.metrics_tracker.latest_gradient_norm:
            grad_norm = self.metrics_tracker.latest_gradient_norm
            grad_status = "âœ…" if grad_norm < 10 else "âš ï¸ "
            print(f"   Gradient Norm:      {grad_norm:.3f} {grad_status} (target: <10)")
            if grad_norm > 10:
                print(f"      â†’ Reduce max_grad_norm or learning_rate")
        
        print(f"\nðŸ’¡ TensorBoard: {self.metrics_tracker.log_dir}")
        print(f"   â†’ Focus on 0_critical/ namespace for hyperparameter tuning")
        print("="*80 + "\n")
    
    def _run_final_bot_eval(self, model, training_config, training_config_name, rewards_config_name):
        """Run final comprehensive bot evaluation using standalone function"""
        controlled_agent = self.controlled_agent  # CRITICAL FIX: Use stored controlled_agent instead of looking in training_config
        
        # Extract n_episodes from callback_params in training_config
        if 'callback_params' not in training_config:
            raise KeyError("training_config missing required 'callback_params' field")
        if 'bot_eval_final' not in training_config['callback_params']:
            raise KeyError("training_config['callback_params'] missing required 'bot_eval_final' field")
        n_episodes = training_config['callback_params']['bot_eval_final']
        
        # Use standalone function with progress bar for final eval
        return evaluate_against_bots(
            model=model,
            training_config_name=training_config_name,
            rewards_config_name=rewards_config_name,
            n_episodes=n_episodes,
            controlled_agent=controlled_agent,
            show_progress=True,
            deterministic=True
        )
    
    def _on_step(self) -> bool:
        """Collect step-level data including actions, damage, and unit changes"""
        # Track step-level reward and length
        if hasattr(self, 'locals'):
            if 'rewards' in self.locals:
                reward = self.locals['rewards'][0] if isinstance(self.locals['rewards'], (list, np.ndarray)) else self.locals['rewards']
                self.episode_reward += reward
            
            self.episode_length += 1
            
            # Process info dict for action tracking
            if 'infos' in self.locals:
                for info in self.locals['infos']:
                    # Track action validity
                    if 'success' in info:
                        if info['success']:
                            self.episode_tactical_data['valid_actions'] += 1
                        else:
                            self.episode_tactical_data['invalid_actions'] += 1
                        
                        self.episode_tactical_data['total_actions'] += 1
                    
                    # Track wait actions (action type in info)
                    if info.get('action') == 'wait' or info.get('action') == 'skip':
                        self.episode_tactical_data['wait_actions'] += 1
                    
                    # Track damage from combat results
                    if 'totalDamage' in info:
                        damage_dealt = info.get('totalDamage', 0)
                        self.episode_tactical_data['damage_dealt'] += damage_dealt
                    
                    # Handle episode end
                    if info.get('episode', False) or info.get('winner') is not None:
                        self._handle_episode_end(info)
        
        # NEW: Collect reward decomposition from game_state
        if hasattr(self.training_env, 'envs') and len(self.training_env.envs) > 0:
            env = self.training_env.envs[0]
            
            if hasattr(env, 'unwrapped') and hasattr(env.unwrapped, 'game_state'):
                game_state = env.unwrapped.game_state
                
                # Collect reward breakdown if available
                if 'last_reward_breakdown' in game_state:
                    reward_breakdown = game_state['last_reward_breakdown']
                    
                    # Accumulate reward components for episode
                    if not hasattr(self, 'episode_reward_components'):
                        self.episode_reward_components = {
                            'base_actions': 0.0,
                            'result_bonuses': 0.0,
                            'tactical_bonuses': 0.0,
                            'situational': 0.0,
                            'penalties': 0.0
                        }
                    
                    self.episode_reward_components['base_actions'] += reward_breakdown.get('base_actions', 0.0)
                    self.episode_reward_components['result_bonuses'] += reward_breakdown.get('result_bonuses', 0.0)
                    self.episode_reward_components['tactical_bonuses'] += reward_breakdown.get('tactical_bonuses', 0.0)
                    self.episode_reward_components['situational'] += reward_breakdown.get('situational', 0.0)
                    self.episode_reward_components['penalties'] += reward_breakdown.get('penalties', 0.0)
                    
                    # Clear breakdown from game_state to avoid double-counting
                    del game_state['last_reward_breakdown']
        
        # Simple Q-value tracking every 100 steps
        if self.model.num_timesteps % 100 == 0 and hasattr(self.model, 'q_net'):
            try:
                # Use a simple dummy observation if locals not available
                if hasattr(self, 'locals') and 'obs' in self.locals:
                    obs_tensor = torch.FloatTensor(self.locals['obs']).to(self.model.device)
                else:
                    # Create dummy observation matching env observation space
                    dummy_obs = torch.zeros((1, self.model.observation_space.shape[0])).to(self.model.device)
                    obs_tensor = dummy_obs
                
                with torch.no_grad():
                    q_values = self.model.q_net(obs_tensor)
                    mean_q_value = q_values.mean().item()
                    
                    # Track history
                    self.q_value_history.append(mean_q_value)
                    if len(self.q_value_history) > self.max_q_value_history:
                        self.q_value_history.pop(0)
                    
                    # Calculate smoothed mean
                    q_value_mean = sum(self.q_value_history) / len(self.q_value_history)
                    
                    # Log single metrics
                    if hasattr(self.model, 'logger') and self.model.logger:
                        self.model.logger.record('train/q_value_mean_smooth', q_value_mean)
                        self.model.logger.dump(step=self.model.num_timesteps)
            except Exception as e:
                pass  # Q-value tracking is optional
        
        # Log training step data
        step_data = {}
        if hasattr(self.model, 'learning_rate'):
            step_data['learning_rate'] = self.model.learning_rate
        if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
            if 'train/loss' in self.model.logger.name_to_value:
                step_data['loss'] = self.model.logger.name_to_value['train/loss']
        if hasattr(self.model, 'exploration_rate'):
            step_data['exploration_rate'] = self.model.exploration_rate
        
        if step_data:
            self.metrics_tracker.log_training_step(step_data)
        
        # NEW: Log PPO hyperparameter metrics from stable-baselines3
        # Extract all available training metrics for hyperparameter tuning
        if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
            model_stats = self.model.logger.name_to_value
            
            # Only log if there are actual training metrics available
            # SB3 updates these after each policy update (every n_steps)
            if len(model_stats) > 0:
                # Pass complete model stats to metrics tracker
                # This includes: learning_rate, policy_loss, value_loss, entropy_loss,
                # clip_fraction, approx_kl, explained_variance, n_updates, fps
                self.metrics_tracker.log_training_metrics(model_stats)
                
                # Update step count for proper metric indexing
                self.metrics_tracker.step_count = self.model.num_timesteps
        
        return True
    
    def _handle_episode_end(self, info):
        """Handle episode completion and log metrics."""
        self.episode_count += 1

        # Extract episode data
        episode_data = {
            'total_reward': info.get('episode_reward', self.episode_reward),
            'episode_length': info.get('episode_length', self.episode_length),
            'winner': info.get('winner', None)
        }

        # DIAGNOSTIC: Log winner for first 10 episodes (disabled for cleaner output)
        # if self.episode_count <= 10:
        #     winner = episode_data['winner']
        #     print(f"  [DIAG] Episode {self.episode_count}: winner={winner} (P0 wins if 0, P1 wins if 1, draw if -1)")
       
        # GAMMA MONITORING: Track discount factor effects
        if hasattr(self.model, 'gamma'):
            gamma = self.model.gamma
           
            # Calculate temporal metrics
            immediate_reward_ratio = self._calculate_immediate_vs_future_ratio(info)
            planning_horizon = self._estimate_planning_horizon(gamma)
           
            # Track ratio history for smoothing
            self.immediate_reward_ratio_history.append(immediate_reward_ratio)
            if len(self.immediate_reward_ratio_history) > self.max_reward_ratio_history:
                self.immediate_reward_ratio_history.pop(0)
           
            # Calculate smoothed mean
            ratio_mean = sum(self.immediate_reward_ratio_history) / len(self.immediate_reward_ratio_history)
           
            # Log gamma-related metrics
            if hasattr(self.model, 'logger') and self.model.logger:
                self.model.logger.record('config/discount_factor', gamma)
                self.model.logger.record('config/immediate_reward_ratio', immediate_reward_ratio)
                self.model.logger.record('config/immediate_reward_ratio_mean', ratio_mean)
                self.model.logger.record('config/planning_horizon', planning_horizon)
                # Force tensorboard dump to ensure gamma metrics are written
                self.model.logger.dump(step=self.model.num_timesteps)
       
        # CRITICAL: Use tactical_data from engine (populated during episode)
        if 'tactical_data' in info:
            # Engine provides complete tactical data - use it directly
            self.episode_tactical_data.update(info['tactical_data'])
       
        # Log to metrics tracker (KEEP for state tracking)
        self.metrics_tracker.log_episode_end(episode_data)
        self.metrics_tracker.log_tactical_metrics(self.episode_tactical_data)
        
        # CRITICAL FIX: Write game_critical metrics directly to model.logger
        # This ensures metrics appear in same TensorBoard directory as train/ metrics
        if hasattr(self.model, 'logger') and self.model.logger:
            total_reward = episode_data.get('total_reward', 0)
            episode_length = episode_data.get('episode_length', 0)
            winner = episode_data.get('winner', None)
            
            # Game critical metrics
            self.model.logger.record('game_critical/episode_reward', total_reward)
            self.model.logger.record('game_critical/episode_length', episode_length)
            
            # Win rate calculation (need rolling window)
            if not hasattr(self, 'win_rate_window'):
                from collections import deque
                self.win_rate_window = deque(maxlen=100)
            
            if winner is not None:
                # CRITICAL FIX: Learning agent is Player 0, not Player 1!
                agent_won = 1.0 if winner == 0 else 0.0
                self.win_rate_window.append(agent_won)

                if len(self.win_rate_window) >= 10:
                    import numpy as np
                    rolling_win_rate = np.mean(self.win_rate_window)
                    self.model.logger.record('game_critical/win_rate_100ep', rolling_win_rate)
            
            # Tactical metrics
            units_killed = self.episode_tactical_data.get('units_killed', 0)
            units_lost = max(self.episode_tactical_data.get('units_lost', 0), 1)  # Avoid division by zero
            kill_loss_ratio = units_killed / units_lost
            self.model.logger.record('game_critical/units_killed_vs_lost_ratio', kill_loss_ratio)
            
            # Invalid action rate
            total_actions = self.episode_tactical_data.get('total_actions', 0)
            if total_actions > 0:
                invalid_rate = self.episode_tactical_data.get('invalid_actions', 0) / total_actions
                self.model.logger.record('game_critical/invalid_action_rate', invalid_rate)
            
            # Dump metrics to TensorBoard
            self.model.logger.dump(step=self.model.num_timesteps)
        
        # NEW: Log reward decomposition
        if hasattr(self, 'episode_reward_components'):
            self.metrics_tracker.log_reward_decomposition(self.episode_reward_components)
            # Reset for next episode
            self.episode_reward_components = {
                'base_actions': 0.0,
                'result_bonuses': 0.0,
                'tactical_bonuses': 0.0,
                'situational': 0.0,
                'penalties': 0.0
            }
       
        # Reset episode tracking with ALL fields
        self.episode_reward = 0
        self.episode_length = 0
        self.episode_tactical_data = {
            # Combat metrics
            'shots_fired': 0,
            'hits': 0,
            'total_enemies': 0,
            'killed_enemies': 0,
            
            # Damage tracking
            'damage_dealt': 0,
            'damage_received': 0,
            
            # Unit tracking
            'units_lost': 0,
            'units_killed': 0,
            
            # Action tracking
            'valid_actions': 0,
            'invalid_actions': 0,
            'wait_actions': 0,
            'total_actions': 0,
            
            # Phase tracking
            'phases_completed': 0,
            'total_phases': 6
        }
    
    def _calculate_immediate_vs_future_ratio(self, info):
        """Calculate ratio of immediate vs future-oriented actions"""
        # Analyze action patterns to detect myopic vs strategic behavior
        immediate_actions = 0  # Shooting, direct attacks
        future_actions = 0     # Movement, positioning
        
        if hasattr(self.training_env, 'envs') and len(self.training_env.envs) > 0:
            env = self.training_env.envs[0]
            if hasattr(env, 'unwrapped') and hasattr(env.unwrapped, 'game_state'):
                action_logs = env.unwrapped.game_state.get('action_logs', [])
                
                for log in action_logs:
                    action_type = log.get('type', '')
                    if action_type in ['shoot', 'combat']:
                        immediate_actions += 1
                    elif action_type in ['move', 'wait']:
                        future_actions += 1
        
        total_actions = immediate_actions + future_actions
        return immediate_actions / max(1, total_actions)
    
    def _estimate_planning_horizon(self, gamma):
        """Estimate effective planning horizon from discount factor"""
        # Planning horizon = how many turns ahead agent effectively considers
        # Formula: horizon â‰ˆ 1 / (1 - gamma)
        if gamma >= 0.99:
            return float('inf')  # Very long-term planning
        else:
            return 1.0 / (1.0 - gamma)

