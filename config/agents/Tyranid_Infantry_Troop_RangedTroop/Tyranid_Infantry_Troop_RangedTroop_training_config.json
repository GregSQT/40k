{
  "default": {
  "type": "################################################### Default ###################################################",
  "total_episodes": 30000,
  "total_episodes_normal": "4000 - 100 ep / minute -> 6k ep / hour",
  "max_turns_per_episode": 5,
  "n_envs": 48,
  "callback_params": {
    "checkpoint_save_freq": 50000,
    "checkpoint_name_prefix": "ppo_checkpoint",
    "eval_deterministic": true,
    "eval_render": false,
    "bot_eval_freq": 1000,
    "bot_eval_use_episodes": true,
    "bot_eval_intermediate": 200,
    "bot_eval_final": 0,
    "save_best_robust": true,
    "robust_window": 5,
    "robust_drawdown_penalty": 0.5,
    "bot_eval_weights": {"random": 0.2, "greedy": 0.4, "defensive": 0.4},
    "bot_eval_randomness": {"greedy": 0.1, "defensive": 0.1}
  },
    "observation_params": {
      "obs_size": 323,
    "perception_radius": 25,
    "max_nearby_units": 10,
    "max_valid_targets": 5,
    "justification": "R=25 covers MOVE(12)+CHARGE(12)+ENEMY_OFFSET(1). 314 floats = 16 global (added has_advanced) + 22 unit capabilities + 32 terrain + 72 ally + 132 enemy (6×22) + 40 targets (5×8) - ADVANCE_IMPLEMENTATION_PLAN.md"
  },
  "step_log_buffer_size": 200,
  "model_params": {
    "policy": "MlpPolicy",
    "learning_rate": {"initial": 0.0002, "final": 0.00006},
    "n_steps": 16384,
    "batch_size": 4096,
    "n_epochs": 5,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.12,
    "clip_range_vf": null,
    "normalize_advantage": true,
    "ent_coef": { "start": 0.04, "end": 0.02 },
    "vf_coef": 1.0,
    "max_grad_norm": 0.5,
    "target_kl": 0.01,
    "tensorboard_log": "./tensorboard/",
    "policy_kwargs": {
      "net_arch": [320, 320]
    },
    "verbose": 0
  },
  "vec_normalize": {
    "enabled": true,
    "norm_obs": true,
    "norm_reward": true,
    "clip_obs": 10.0,
    "clip_reward": 10.0,
    "gamma": 0.99
  },
  "vec_normalize_eval": {
    "enabled": true,
    "training": false,
    "norm_reward": false
  },
  "bot_training": {
    "ratios": {"random": 0.25, "greedy": 0.375, "defensive": 0.375},
    "greedy_randomness": 0.1,
    "defensive_randomness": 0.1
  }
},
  "stabilize": {
  "type": "############################################### Stabilize (Phase 2) ###############################################",
  "total_episodes": 10000,
  "total_episodes_normal": "Phase 2 append anti-regression",
  "max_turns_per_episode": 5,
  "n_envs": 48,
  "callback_params": {
    "checkpoint_save_freq": 50000,
    "checkpoint_name_prefix": "ppo_checkpoint",
    "eval_deterministic": true,
    "eval_render": false,
    "bot_eval_freq": 500,
    "bot_eval_use_episodes": true,
    "bot_eval_intermediate": 50,
    "bot_eval_final": 100,
    "save_best_robust": true,
    "robust_window": 3,
    "robust_drawdown_penalty": 0.5,
    "bot_eval_weights": {"random": 0.2, "greedy": 0.4, "defensive": 0.4},
    "bot_eval_randomness": {"greedy": 0.15, "defensive": 0.15}
  },
    "observation_params": {
      "obs_size": 323,
    "perception_radius": 25,
    "max_nearby_units": 10,
    "max_valid_targets": 5,
    "justification": "R=25 covers MOVE(12)+CHARGE(12)+ENEMY_OFFSET(1). 314 floats = 16 global (added has_advanced) + 22 unit capabilities + 32 terrain + 72 ally + 132 enemy (6×22) + 40 targets (5×8) - ADVANCE_IMPLEMENTATION_PLAN.md"
  },
  "step_log_buffer_size": 200,
  "model_params": {
    "policy": "MlpPolicy",
    "learning_rate": {"initial": 0.00012, "final": 0.00003},
    "n_steps": 16384,
    "batch_size": 4096,
    "n_epochs": 4,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.12,
    "clip_range_vf": null,
    "normalize_advantage": true,
    "ent_coef": 0.02,
    "vf_coef": 1.0,
    "max_grad_norm": 0.5,
    "target_kl": 0.008,
    "tensorboard_log": "./tensorboard/",
    "policy_kwargs": {
      "net_arch": [320, 320]
    },
    "verbose": 0
  },
  "vec_normalize": {
    "enabled": true,
    "norm_obs": true,
    "norm_reward": true,
    "clip_obs": 10.0,
    "clip_reward": 10.0,
    "gamma": 0.99
  },
  "vec_normalize_eval": {
    "enabled": true,
    "training": false,
    "norm_reward": false
  },
  "bot_training": {
    "ratios": {"random": 0.25, "greedy": 0.375, "defensive": 0.375},
    "greedy_randomness": 0.15,
    "defensive_randomness": 0.15
  }
},
  "debug": {
    "type": "################################################### debug ###################################################",
    "total_episodes": 10,
    "total_episodes_normal": "4000 - 100 ep / minute -> 6k ep / hour",
    "max_turns_per_episode": 5,
    "n_envs": 1,
    "callback_params": {
      "checkpoint_save_freq": 50000,
      "checkpoint_name_prefix": "ppo_checkpoint",
      "eval_deterministic": true,
      "eval_render": false,
      "n_eval_episodes": 5,
      "bot_eval_freq": 10000,
      "bot_eval_use_episodes": true,
      "bot_eval_intermediate": 10,
      "bot_eval_final": 0,
      "bot_eval_weights": {"random": 0.35, "greedy": 0.30, "defensive": 0.35},
      "bot_eval_randomness": {"greedy": 0.15, "defensive": 0.15}
    },
    "observation_params": {
      "obs_size": 323,
      "perception_radius": 25,
      "max_nearby_units": 10,
      "max_valid_targets": 5,
      "justification": "R=25 covers MOVE(12)+CHARGE(12)+ENEMY_OFFSET(1). 314 floats = 16 global (added has_advanced) + 22 unit capabilities + 32 terrain + 72 ally + 132 enemy (6×22) + 40 targets (5×8) - ADVANCE_IMPLEMENTATION_PLAN.md"
    },
    "exploration": {
      "initial_epsilon": 0.3,
      "final_epsilon": 0.05,
      "exploration_fraction": 0.3 
    },
    "step_log_buffer_size": 200,
    "model_params": {
      "policy": "MlpPolicy",
      "learning_rate": 0.0001,
      "n_steps": 256,
      "batch_size": 256,
      "n_epochs": 10,
      "gamma": 0.95,
      "gae_lambda": 0.95,
      "clip_range": 0.2,
      "clip_range_vf": null,
      "normalize_advantage": true,
      "ent_coef": 0.10,
      "vf_coef": 1.0,
      "max_grad_norm": 0.5,
      "target_kl": 0.01,
      "tensorboard_log": "./tensorboard/",
      "policy_kwargs": {
        "net_arch": [320, 320]
      },
      "verbose": 0
    },
    "vec_normalize": {
      "enabled": true,
      "norm_obs": true,
      "norm_reward": true,
      "clip_obs": 10.0,
      "clip_reward": 10.0,
      "gamma": 0.99
    },
    "vec_normalize_eval": {
      "enabled": true,
      "training": false,
      "norm_reward": false
    },
    "bot_training": {
      "ratios": {"random": 0.2, "greedy": 0.4, "defensive": 0.4},
      "greedy_randomness": 0.10,
      "defensive_randomness": 0.10
    }
  }
}
