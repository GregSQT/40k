class EpisodeBasedEvalCallback(BaseCallback):
    """Episode-counting evaluation callback - triggers every N episodes, not timesteps."""
    
    def __init__(self, eval_env, episodes_per_eval=10, n_eval_episodes=5, 
                 best_model_save_path=None, log_path=None, deterministic=True, verbose=0):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.episodes_per_eval = episodes_per_eval
        self.n_eval_episodes = n_eval_episodes
        self.best_model_save_path = best_model_save_path
        self.log_path = log_path
        self.deterministic = deterministic
        
        # Episode tracking
        self.episode_count = 0
        self.last_eval_episode = 0
        self.best_mean_reward = -float('inf')
        self.win_rate_history = []
        self.loss_history = []
        self.q_value_history = []
        self.gradient_norm_history = []
        self.max_history = 20  # Keep last 20 evaluations for smoothing
        self.max_loss_history = 100  # Keep more loss values for better smoothing
        
    def _on_step(self) -> bool:
        # Track training metrics for smoothing
        if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
            logger_data = self.model.logger.name_to_value
            
            # Track loss
            if 'train/loss' in logger_data:
                current_loss = logger_data['train/loss']
                self.loss_history.append(current_loss)
                if len(self.loss_history) > self.max_loss_history:
                    self.loss_history.pop(0)
                
                if len(self.loss_history) > 5:
                    loss_mean = sum(self.loss_history) / len(self.loss_history)
                    self.model.logger.record("train/loss_mean", loss_mean)
            
            # Track Q-values if available
            if hasattr(self.model, 'q_net') and hasattr(self, 'locals') and 'obs' in self.locals:
                try:
                    import torch
                    obs_tensor = torch.FloatTensor(self.locals['obs']).to(self.model.device)
                    with torch.no_grad():
                        q_values = self.model.q_net(obs_tensor)
                        mean_q_value = q_values.mean().item()
                        self.q_value_history.append(mean_q_value)
                        if len(self.q_value_history) > self.max_loss_history:
                            self.q_value_history.pop(0)
                        
                        if len(self.q_value_history) > 5:
                            q_value_mean = sum(self.q_value_history) / len(self.q_value_history)
                            self.model.logger.record("train/q_value_mean", q_value_mean)
                except Exception:
                    pass  # Q-value tracking is optional
            
            # Track gradient norm if available
            if hasattr(self.model, 'policy') and hasattr(self.model.policy, 'parameters'):
                try:
                    total_norm = 0.0
                    param_count = 0
                    for p in self.model.policy.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                            param_count += 1
                    
                    if param_count > 0:
                        gradient_norm = (total_norm ** 0.5)
                        self.gradient_norm_history.append(gradient_norm)
                        if len(self.gradient_norm_history) > self.max_loss_history:
                            self.gradient_norm_history.pop(0)
                        
                        if len(self.gradient_norm_history) > 5:
                            grad_norm_mean = sum(self.gradient_norm_history) / len(self.gradient_norm_history)
                            self.model.logger.record("train/gradient_norm", grad_norm_mean)
                except Exception:
                    pass  # Gradient tracking is optional
            
            # Dump all metrics
            if len(self.loss_history) > 5:
                self.model.logger.dump(step=self.model.num_timesteps)
        
        # Check for episode completion
        if hasattr(self, 'locals') and 'dones' in self.locals and 'infos' in self.locals:
            for i, done in enumerate(self.locals['dones']):
                if done and i < len(self.locals['infos']):
                    info = self.locals['infos'][i]
                    if 'episode' in info:
                        self.episode_count += 1
                        
                        # Check if it's time for evaluation
                        episodes_since_eval = self.episode_count - self.last_eval_episode
                        if episodes_since_eval >= self.episodes_per_eval:
                            self._run_evaluation()
                            self.last_eval_episode = self.episode_count
                            
        return True
    
    def _run_evaluation(self):
        """Run evaluation episodes and log results to tensorboard."""
        episode_rewards = []
        episode_lengths = []
        wins = 0
        
        for eval_episode in range(self.n_eval_episodes):
            obs, info = self.eval_env.reset()
            episode_reward = 0
            episode_length = 0
            done = False
            final_info = None
            
            while not done and episode_length < 1000:  # Prevent infinite loops
                action, _ = self.model.predict(obs, deterministic=self.deterministic)
                obs, reward, terminated, truncated, info = self.eval_env.step(action)
                episode_reward += reward
                episode_length += 1
                done = terminated or truncated
                if done:
                    final_info = info
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # Track wins - check if AI (player 1) won
            if final_info and final_info.get('winner') == 1:
                wins += 1
        
        # Calculate statistics
        mean_reward = sum(episode_rewards) / len(episode_rewards)
        mean_ep_length = sum(episode_lengths) / len(episode_lengths)
        win_rate = wins / self.n_eval_episodes if self.n_eval_episodes > 0 else 0.0
        
        # Track win rate history for smoothing
        self.win_rate_history.append(win_rate)
        if len(self.win_rate_history) > self.max_history:
            self.win_rate_history.pop(0)
        
        # Calculate smoothed win rate (mean of recent evaluations)
        win_rate_mean = sum(self.win_rate_history) / len(self.win_rate_history)
        
        # Log to model's tensorboard logger directly
        if hasattr(self.model, 'logger') and self.model.logger:
            self.model.logger.record("eval/mean_reward", mean_reward)
            self.model.logger.record("eval/mean_ep_length", mean_ep_length)
            self.model.logger.record("eval/win_rate", win_rate)
            self.model.logger.record("eval/win_rate_mean", win_rate_mean)
            self.model.logger.record("eval/episode", self.episode_count)
            self.model.logger.dump(step=self.model.num_timesteps)
        else:
            # Fallback to callback logger
            self.logger.record("eval/mean_reward", mean_reward)
            self.logger.record("eval/mean_ep_length", mean_ep_length)
            self.logger.record("eval/win_rate", win_rate)
            self.logger.dump(step=self.num_timesteps)
        
        # if self.verbose > 0:
            # print(f"Episode {self.episode_count}: Eval mean reward: {mean_reward:.2f}, Win rate: {win_rate:.1%}")
        
        # Save best model
        if mean_reward > self.best_mean_reward:
            self.best_mean_reward = mean_reward
            if self.best_model_save_path:
                self.model.save(f"{self.best_model_save_path}/best_model")
        
        # if self.verbose > 0:
            # print(f"Episode {self.episode_count}: Eval mean reward: {mean_reward:.2f}")

